# PokemonUltimate - Cursor AI Rules

> These rules are automatically loaded by Cursor for every conversation.

## ğŸš€ Automatic Context Loading

**ALWAYS read these files at the start of any task:**
1. `.ai/context.md` - Current project state, phase, completed systems
2. `docs/project_guidelines.md` - 24+ mandatory coding rules

**Read on-demand based on task:**
- New feature â†’ `docs/architecture/[relevant].md`
- Code quality â†’ `docs/anti-patterns.md`
- Examples needed â†’ `docs/examples/good_code.md`

## ğŸ“‹ Development Workflow

### When User Says "Implement X" or "Add X"
**MANDATORY WORKFLOW - Follow ALL steps in order:**

1. **Read Context & Specs**
   - Read `.ai/context.md` to understand current state
   - **CRITICAL: Read relevant architecture doc in `docs/architecture/`**
   - If spec is incomplete or missing details, **COMPLETE IT FIRST**
   - Identify ALL requirements from the spec
   - Note any elements to defer to later phases
   - Understand the expected API (method names, parameters)

2. **Verify Spec Completeness**
   - List what will be implemented vs deferred
   - Ensure spec has all necessary details (interfaces, classes, methods, examples)
   - If spec is incomplete, update it before implementation
   - Document any API changes from spec (with rationale)

3. **TDD: Write Functional Tests FIRST**
   - **CRITICAL: Follow test structure in `docs/testing/test_structure_definition.md`**
   - **Test Location**: 
     - System tests â†’ `Systems/[Module]/[Component]Tests.cs`
     - Blueprint tests â†’ `Blueprints/[Blueprint]Tests.cs`
     - Data tests â†’ `Data/Pokemon/[Pokemon]Tests.cs`, `Data/Moves/[Move]Tests.cs`, etc.
     - Catalog tests â†’ `Data/Catalogs/[Catalog]Tests.cs` (general catalog tests)
   - Create test file following naming: `[Feature]Tests.cs`
   - Write tests for ALL main scenarios from the spec
   - Use naming: `MethodName_Scenario_ExpectedResult`
   - Tests should compile but fail (red phase)

4. **Implement Feature**
   - Follow the spec exactly
   - Use existing patterns from codebase
   - Follow all coding rules (no magic strings, fail-fast, etc.)
   - Make tests pass (green phase)

5. **Write Edge Case Tests**
   - Create `[Feature]EdgeCasesTests.cs` in same directory as functional tests
   - Test null inputs, boundary conditions, invalid states
   - Test real-world scenarios
   - **If tests reveal missing functionality â†’ implement it immediately** (Test-Driven Discovery)

6. **Write Integration Tests** (MANDATORY if feature interacts with multiple systems)
   - **When Required:**
     - Feature creates actions that interact with BattleQueue
     - Feature modifies state that affects other systems
     - Feature is a system boundary (e.g., Status â†’ End-of-Turn â†’ Damage)
     - Feature coordinates multiple components (e.g., CombatEngine â†’ TurnOrderResolver â†’ BattleQueue)
   - **What to Test:**
     - System interactions (A â†’ B â†’ C)
     - Cascading effects (Action â†’ Reaction â†’ Outcome)
     - State consistency across systems
   - **Where:** Create `[Feature]IntegrationTests.cs` in `Systems/[Module]/Integration/[Category]/`
     - Actions integration â†’ `Systems/Combat/Integration/Actions/`
     - Damage integration â†’ `Systems/Combat/Integration/Damage/`
     - Engine integration â†’ `Systems/Combat/Integration/Engine/`
     - System integration â†’ `Systems/Combat/Integration/System/`
   - **Naming:** `[System1]_[System2]_[ExpectedBehavior]`
   - **If Unsure:** Ask "Does this feature work with other systems?" If yes â†’ Write integration tests

7. **Validate Against Use Cases** (Combat features only)
   - Read relevant sections of `docs/combat_use_cases.md`
   - Verify implementation covers all listed behaviors
   - Mark items as complete in the use cases document
   - Document any deferred items with rationale
   - **If use case reveals missing functionality â†’ implement it immediately**

8. **Verify Implementation**
   - Run `dotnet build` - Verify no warnings
   - Run `dotnet test` - Verify all tests pass
   - Check `docs/checklists/feature_complete.md` for final verification
   - Mention test count in response

9. **Update Documentation**
   - **Immediate Updates** (before moving on):
     - Update `.ai/context.md` with new state (phase, test count, completed systems)
     - Mark use cases as complete in `docs/combat_use_cases.md` (if applicable)
     - Update test count in response
   
   - **If API Changed:**
     - Update relevant architecture doc in `docs/architecture/`
     - Note changes in spec document with rationale
   
   - **If New Pattern Introduced:**
     - Create new architecture doc OR update existing pattern doc
     - Document rationale in `.ai/context.md` under "Key Architectural Decisions"
   
   - **Status Documents:**
     - Update phase-specific status docs (e.g., `player_input_status.md`) if they exist
     - Update implementation plan if phase completed

### When User Says "Review X" or "Check X"
1. Read `docs/anti-patterns.md`
2. Verify against `docs/project_guidelines.md`
3. Check `docs/checklists/feature_complete.md` for completeness
4. List issues found
5. Fix issues if requested

### When User Says "Edge Cases" or "Test X"
1. Read `docs/examples/good_tests.md` for patterns
2. Apply boundary testing (min/max values)
3. Test invalid inputs (null, negative, overflow)
4. Add real-world verification if applicable
5. If tests reveal missing functionality â†’ implement it

### When User Says "Refactor X" or "Improve X"
1. **Identify Scope**
   - Read current implementation
   - Identify what needs improvement
   - Check `docs/anti-patterns.md` for violations
   - Verify tests exist (if not, write them first using TDD)

2. **Refactor Safely**
   - Make small, incremental changes
   - Run tests after each change
   - Maintain API compatibility (or document breaking changes)
   - Follow existing patterns from codebase

3. **Update Documentation**
   - Update architecture docs if pattern changed
   - Update `.ai/context.md` if significant
   - Document rationale for changes

### When Issues Arise During Implementation

**If Spec is Incomplete:**
1. Stop implementation
2. Complete the spec first in `docs/architecture/`
3. Document what was missing
4. Resume implementation

**If Spec is Incorrect:**
1. Document the discrepancy
2. Decide: Fix spec or change implementation?
3. If changing implementation, update spec to match
4. Note decision in `.ai/context.md` under "Key Architectural Decisions"

**If Discovery Requires Architectural Change:**
1. Pause current work
2. Document the discovery
3. Evaluate impact on other systems
4. Update architecture docs if needed
5. Resume with updated understanding

**If Test Reveals Missing Functionality:**
1. **DO NOT** skip the test or mark it as "future work"
2. Implement the missing functionality immediately (Test-Driven Discovery)
3. Update spec/documentation if needed
4. Continue until feature is complete

## âœ… Mandatory Rules (Always Apply)

### Code Quality
- **NO magic strings** â†’ Use `ErrorMessages.cs` or `GameMessages.cs`
- **NO magic numbers** â†’ Use named constants
- **NO try-catch** â†’ Unless absolutely necessary (I/O, external APIs)
- **Fail-fast** â†’ Throw exceptions for invalid inputs
- **Guard clauses** â†’ Validate at method start

### Architecture
- **Core/** â†’ Logic only, NO game data
- **Content/** â†’ Game data, catalogs, builders
- **Tests/** â†’ Organized by purpose: Systems/, Blueprints/, Data/
  - **Systems/** â†’ Tests de sistemas (CÃ“MO funcionan)
  - **Blueprints/** â†’ Tests de estructura de datos (CÃ“MO son)
  - **Data/** â†’ Tests de contenido especÃ­fico (QUÃ‰ contienen)
- **Blueprints** â†’ Immutable (no setters)
- **Instances** â†’ Mutable runtime state

### Testing
- **Test Structure** â†’ MUST follow `docs/testing/test_structure_definition.md`
  - **Systems/** â†’ Tests de sistemas (CÃ“MO funcionan)
  - **Blueprints/** â†’ Tests de estructura de datos (CÃ“MO son)
  - **Data/** â†’ Tests de contenido especÃ­fico (QUÃ‰ contienen)
- **Test Types**:
  - Functional: `*Tests.cs` - Comportamiento normal
  - Edge Cases: `*EdgeCasesTests.cs` - Casos lÃ­mite
  - Integration: `*IntegrationTests.cs` - IntegraciÃ³n entre sistemas (en `Systems/*/Integration/`)
- **Data Organization** â†’ Un archivo por cada elemento en `Data/Pokemon/`, `Data/Moves/`, etc.
  - Tests generales de catÃ¡logos en `Data/Catalogs/` (PokemonCatalogTests.cs, MoveCatalogTests.cs)
- **TDD** â†’ Tests before implementation
- **Naming** â†’ `MethodName_Scenario_ExpectedResult`
- **Three-phase** â†’ Functional tests, then edge cases, then integration tests
- **Real-world** â†’ Verify against official game data when applicable
- **Test-Driven Discovery** â†’ If test reveals missing functionality, implement it

### Documentation
- **XML docs** â†’ All public APIs
- **Update context** â†’ After major features
- **Document decisions** â†’ Architectural changes go in `.ai/context.md`

## ğŸ” Quick Reference

### Exception Messages
```csharp
// âœ… Correct
throw new ArgumentException(ErrorMessages.AmountCannotBeNegative, nameof(amount));

// âŒ Wrong
throw new Exception("Amount cannot be negative");
```

### Test Pattern
```csharp
[Test]
public void MethodName_Scenario_ExpectedResult()
{
    // Arrange
    // Act
    // Assert
}
```

### Validation Pattern
```csharp
public void Method(int value)
{
    if (value < 0)
        throw new ArgumentException(ErrorMessages.ValueCannotBeNegative, nameof(value));
    
    // Main logic
}
```

## ğŸ“ Key Files Reference

| Need | Read |
|------|------|
| Project state | `.ai/context.md` |
| Coding rules | `docs/project_guidelines.md` |
| What NOT to do | `docs/anti-patterns.md` |
| Code examples | `docs/examples/good_code.md` |
| Test examples | `docs/examples/good_tests.md` |
| Pre-implementation | `docs/checklists/pre_implementation.md` |
| Feature checklist | `docs/checklists/feature_complete.md` |
| Combat specs | `docs/architecture/combat_system_spec.md` |
| Damage formula | `docs/architecture/damage_and_effect_system.md` |
| Integration tests | `docs/testing/integration_testing_guide.md` |
| Test structure | `docs/testing/test_structure_definition.md` |
| Test reorganization | `docs/testing/test_reorganization_implementation_task.md` |
| Troubleshooting | `docs/workflow/troubleshooting.md` |
| Refactoring | `docs/workflow/refactoring_guide.md` |

## ğŸ”„ After Completing Work

Always:
1. Run `dotnet build` - Verify no warnings
2. Run `dotnet test` - Verify all pass
3. Update `.ai/context.md` if major changes
4. Mention test count in response
5. Verify against `docs/checklists/feature_complete.md`

## âš¡ Quick Verification Checklist

Before marking feature complete, verify:
- [ ] All tests pass (`dotnet test`)
- [ ] No warnings (`dotnet build`)
- [ ] `.ai/context.md` updated
- [ ] Use cases validated (if combat-related)
- [ ] Integration tests written (if applicable)
- [ ] Documentation updated
- [ ] Test count mentioned in response

## ğŸ® Project-Specific Knowledge

### Type Effectiveness
- Gen 6+ chart (Fairy included)
- STAB = 1.5x multiplier
- Dual-type multiplies

### Stat Calculation  
- Gen 3+ formula
- HP formula different from other stats
- Nature = 0.9, 1.0, or 1.1 modifier

### Pokemon Data
- Blueprints in `Content/Catalogs/Pokemon/`
- Moves in `Content/Catalogs/Moves/`
- Effects implement `IMoveEffect`

### Current Phase
- âœ… Phase 1: Core Data (Complete)
- âœ… Phase 2: Instances (Complete)  
- ğŸ¯ Phase 3: Combat System (In Progress)
